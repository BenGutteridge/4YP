{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational GMM.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEo-pk9YGmIj"
      },
      "source": [
        "This notebook is based on Chapter 9 (?) of Bishop, and aims to replicate the 'variational mixture of Gaussians' example worked through there. This is to help my understanding of this example, as much of it is the basis for the gradient ascent approach used in my 4YP, and to hopefully get functioning posterior inference. I'm stuck in a rut, and better to go from a working model than try and debug a crummy bloated one built from scratch that has never worked!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB2Ryd0bGRL7"
      },
      "source": [
        "# Intro\r\n",
        "Distribution of $Z$:\r\n",
        "\r\n",
        "i.e. the assignment $z_i$ for a single $x_i$ equals $k$ with probability $\\pi_k$, where $\\sum \\limits^K \\pi_k = 1$\r\n",
        "\r\n",
        "$p(Z|\\pi) = \\prod \\limits^N \\prod \\limits^K \\pi_{k}^{z_{nk}}$\r\n",
        "\r\n",
        "Conditional distribution of observed data (i.e. dataset given true labels and Gaussian means/precisions):\r\n",
        "\r\n",
        "$p(X|Z,\\mu,\\Lambda) = \\prod \\limits^N \\prod \\limits^K \\mathcal{N}(x_n | \\mu_k, \\Lambda_k^{-1})^{z_{nk}}$\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bPHzCamh00L"
      },
      "source": [
        "#### The joint distribution\r\n",
        "\r\n",
        "Factorises as follows:\r\n",
        "\r\n",
        "$p(X,Z,\\pi,\\mu,\\Lambda) = p(X|Z,\\mu,\\Lambda)p(Z|\\pi)p(\\pi)p(\\mu|\\Lambda)p(\\Lambda)$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfSuheDXiV1Z"
      },
      "source": [
        "#### The variational distribution\r\n",
        "\r\n",
        "Takes the following form - the only assumption is that it factorises between latent variablesd $Z$ and the parameters:\r\n",
        "\r\n",
        "$q(Z,\\pi,\\mu,\\Lambda) = q(Z)q(\\pi,\\mu,\\Lambda)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z1Knjwxi7GD"
      },
      "source": [
        "The fundamental general expression for the iterative variaitional approach here is:\r\n",
        "\r\n",
        "$\\log q_j^*(Z_j) = \\mathbb{E}_{i \\neq j}[\\log p(X,Z)] + c$\r\n",
        "\r\n",
        "i.e. look at the expectation of the joint of $\\log p$ for all the latent variables except the one we are predicting - in our case for considering just $Z$ this is:\r\n",
        "\r\n",
        "$\\log q^*(Z) = \\mathbb{E}_{\\pi,\\mu,\\Lambda}[\\log p(X,Z,\\pi,\\mu,\\Lambda)] + c$\r\n",
        "\r\n",
        "(N.b. when we write $\\mathbb{E}_{\\pi,\\mu,\\Lambda}$, what this really is is $\\mathbb{E}_{q(\\pi,\\mu,\\Lambda)}$)\r\n",
        "\r\n",
        "And then decompose the joint into $\\mathbb{E}_\\pi[\\log p(Z|\\pi)] + \\mathbb{E}_{\\mu,\\Lambda}[\\log p(X|Z,\\mu,\\Lambda)] + c$\r\n",
        "\r\n",
        "Some reworking and taking the exponent again brings us to:\r\n",
        "\r\n",
        "$q^*(Z) = \\prod \\limits^N \\prod \\limits^K r_{nk}^{z_{nk}}$\r\n",
        "where $r_{nk} = \\dfrac{\\rho_{nk}}{\\sum \\limits^K_{j=1} \\rho_{nj}}$ and $\\rho_{nk} = \\pi_k \\mathcal{N}(x_n|\\mu,\\Lambda_k^{-1})$\r\n",
        "\r\n",
        "i.e. we have derived that the form of the $Z$ factor of the variaitonal distribution takes the form of a multinomial, with a responsibility of each mixture component $k$ for each datapoint $n$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxS9aLnxnMpS"
      },
      "source": [
        "This still leaves the other factor of the variational distribution, $q(\\pi,\\mu,\\Lambda)$. Again using the general idea of the expectation over the joint,\r\n",
        "\r\n",
        "$\\log q^*(\\pi,\\mu,\\Lambda) = \\mathbb{E}_Z[\\log p(X,Z,\\pi,\\mu,\\Lambda)] =\\\\ \\mathbb{E}_Z[\\log p(X|Z,\\mu,\\Lambda) + \\log p(Z|\\pi) + \\log p(\\pi) + \\log p(\\mu,\\Lambda)]$\r\n",
        "\r\n",
        "The last two terms simply equal themselves as they don't have a $Z$ component (can go outside of integral and the expectiation is simply an integral over a distribution summing to 1), but the first two have an expectation.\r\n",
        "\r\n",
        "From choice of priors, $p(\\pi)$ is a Dirichlet distribution $= C(\\alpha_0)\\prod \\limits^K \\pi_k^{\\alpha_0-1}$\r\n",
        "\r\n",
        "$p(\\mu,\\Lambda)$ is a factorised product of Gaussian-Wishart: $p(\\mu|\\Lambda)p(\\Lambda) = \\prod \\limits^K \\mathcal{N}(\\mu_k|m_0,(\\beta_0 \\Lambda)^{-1})\\mathcal{W}(\\Lambda_k|W_0,\\nu_0)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp2ckHl6o2tV"
      },
      "source": [
        "#### Gaussian-Wishart parameters\r\n",
        "https://math.stackexchange.com/questions/2803164/degrees-of-freedom-in-a-wishart-distribution\r\n",
        "\r\n",
        "$m_0$ is the 'location' of the mean $\\mu$ chosen usually as zero by symmetry\r\n",
        "\r\n",
        "$\\beta_0$: choosing a large $\\beta_0$ tightens the Gaussian from which $\\mu$ is sampled.\r\n",
        "\r\n",
        "$W_0$ is the positive definite scale matrix\r\n",
        "\r\n",
        "If we have some prior idea of the covariance matrix $\\Lambda^{-1}$, $\\Sigma_0$, we use $\\nu$ and $W$ to encode our confidence in it. We set $W = \\Sigma_0^{-1}$. The mean (expectation) of a Wishart is $\\nu W$ and its mode is $(\\nu-d-1) W$. the least informative prior is $\\nu=d$, or possibly $d+2$ - something small. Choosing a large $\\nu$ encodes confidence in the prior $\\Sigma_0$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hmSvmtLx3cQ"
      },
      "source": [
        "back to the variational distribution. There are clearly two parts, $q(\\pi)$ and $q(\\mu,\\Lambda)$.\r\n",
        "\r\n",
        "$q(\\pi) = \\log p(\\pi) + \\mathbb{E}_Z[\\log p(Z|\\pi)] = \\log C(\\alpha_0) + \\log(\\alpha_0 - 1) \\sum \\limits^K \\pi_k + \\sum \\limits^{N,K} \\mathbb{E}[z_{nk}] \\log \\pi_k$\r\n",
        "\r\n",
        "$\\mathbb{E}_Z[z_{nk}]$ is the expectation of a multinomial, i.e. number of trials*probability, $=r_{nk}$\r\n",
        "\r\n",
        "$q^*(\\pi) = C_1 \\prod \\limits^K \\pi_k^{\\alpha_0-1}\\prod \\limits^N \\pi_k^{r_{nk}}= C_1\\prod \\limits^K \\pi_k^{(\\alpha_0-1+\\sum \\limits^N r_{nk})} = Dir(\\pi|\\alpha)$ where $\\alpha_k = \\alpha_0 + \\sum \\limits^N r_{nk}$\r\n",
        "\r\n",
        "Cool!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji_CQVBP2qpW"
      },
      "source": [
        "And now for $\\log q^*(\\mu,\\Lambda)$:\r\n",
        "\r\n",
        "$\\log q^*(\\mu,\\Lambda) = \\sum \\limits^K \\log p(\\mu_k,\\Lambda_k) + \\mathbb{E}_Z[\\log p(X|Z,\\mu,\\Lambda)] = \\\\\r\n",
        "\\sum \\limits^K \\log (\\mathcal{N}(\\mu_k|m_0,\\beta_0,\\Lambda_k) \\mathcal{W}(\\Lambda_k|\\nu,W_0)) + \\sum \\limits^{N,K} \\mathbb{E}_Z[z_{nk}]\\log \\mathcal{N}(x_n|\\mu_k,\\Lambda_k^{-1})$\r\n",
        "\r\n",
        "Similar to q*(\\pi) this turns into a form of a Gaussian-Wishart with different parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FadB5OQ7C7Uw"
      },
      "source": [
        "!pip install autograd\r\n",
        "import autograd\r\n",
        "import autograd.numpy as np\r\n",
        "from autograd.numpy.linalg import inv, det, multi_dot\r\n",
        "from autograd.scipy.special import digamma, gammaln\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pzdxd2i6OXa"
      },
      "source": [
        "def alpha_k(Nk, alpha0):\r\n",
        "  return alpha0 + Nk\r\n",
        "\r\n",
        "def beta_k(Nk, beta0):\r\n",
        "  return beta0 + Nk\r\n",
        "\r\n",
        "def m_k(Nk, xkbar, betak, m0, beta0):\r\n",
        "  return ((1/betak)*(beta0*m0 + Nk*xkbar)).reshape(2)\r\n",
        "\r\n",
        "def W_k(Nk, xkbar, Sk, m0, beta0):\r\n",
        "  inv_Wk = inv(W0) + Nk*Sk + ((beta0*Nk)/(beta0+Nk))*np.dot((xkbar-m0),(xkbar-m0).T) \r\n",
        "  return inv(inv_Wk)\r\n",
        "\r\n",
        "def nu_k(Nk, nu0):\r\n",
        "  return nu0 + Nk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVWHggFG7svk"
      },
      "source": [
        "These use some new parameters, $N_k, \\bar{x}_k, S_k$, which essentially correspond to the sum of responsibilities, responsibility-weighted mean and responsibility-weighted variance for mixture component $k$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN_ARihE8Jby"
      },
      "source": [
        "def N_k(responsibilities_k):\r\n",
        "  return np.sum(np.array(responsibilities_k))\r\n",
        "\r\n",
        "def x_k_bar(Nk, responsibilities_k, X):\r\n",
        "  sum = np.zeros((1,D))\r\n",
        "  for n in range(N):\r\n",
        "    sum = sum + responsibilities_k[n]*X[n]\r\n",
        "  if Nk > 0:\r\n",
        "    return (1/Nk)*sum\r\n",
        "  else:\r\n",
        "    return 0.\r\n",
        "\r\n",
        "def S_k(Nk, responsibilities_k, X, xkbar):\r\n",
        "  sum = 0.0\r\n",
        "  for n in range(N):\r\n",
        "    sum = sum + responsibilities_k[n]*np.dot((X[n]-xkbar).T,(X[n]-xkbar))\r\n",
        "  if Nk > 0:\r\n",
        "    return (1/Nk)*sum\r\n",
        "  else:\r\n",
        "    return np.eye(2) # doesn't actually matter what it returns, the component is dead"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99bBK_oaLxix"
      },
      "source": [
        "These updates correspond to the M (maximisation) step of the EM algorithm. To perform them we of course need our responsibilities:\r\n",
        "\r\n",
        "$\\mathbb{E}_Z[r_{nk}] = \\dfrac{\\rho_{nk}}{\\sum \\limits^K_{j=1} \\rho_{nj}}$ and\r\n",
        "$\\rho_{nk} = \\pi_k \\mathcal{N}(x_n|\\mu,\\Lambda_k^{-1})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tFnG9OhQRLB"
      },
      "source": [
        "The $\\rho_{nk}$ above is not quite right --- we have ignored the $\\mathbb{E}_{\\mu,\\lambda}[.]$ Let's consider in full:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFkDw6NpOTa0"
      },
      "source": [
        "$\\begin{aligned}\r\n",
        "\\ln \\rho_{n k}=& \\mathbb{E}\\left[\\ln \\pi_{k}\\right]+\\frac{1}{2} \\mathbb{E}\\left[\\ln \\left|\\boldsymbol{\\Lambda}_{k}\\right|\\right]-\\frac{D}{2} \\ln (2 \\pi) \\\\\r\n",
        "&-\\frac{1}{2} \\mathbb{E}_{\\mu_{k}, \\Lambda_{k}}\\left[\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}} \\boldsymbol{\\Lambda}_{k}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\right)\\right]\r\n",
        "\\end{aligned}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6M2u92xQJO5"
      },
      "source": [
        "There are four components here:\r\n",
        "\r\n",
        "\r\n",
        "1.   $\\mathbb{E}[\\ln \\pi_{k}] = \\psi(\\alpha_k) - \\psi(\\sum \\limits^K \\alpha_k)$, using the digamma function $\\psi(x) = \\dfrac{d}{dx}\\log \\Gamma = \\dfrac{\\Gamma '}{\\Gamma}$. This is the known log-expectation of the Dirichlet distribution.\r\n",
        "2.   $ \\mathbb{E}\\left[\\ln \\left|\\boldsymbol{\\Lambda}_{k}\\right|\\right] = \\sum_{i=1}^{D} \\psi\\left(\\frac{\\nu_{k}+1-i}{2}\\right)+D \\ln 2+\\ln \\left|\\mathbf{W}_{k}\\right|$. This is the known log-expectation of the Wishart distribution.\r\n",
        "3. Easy\r\n",
        "4. $\\mathbb{E}_{\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k}}\\left[\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}} \\boldsymbol{\\Lambda}_{k}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\right)\\right] = D\\beta_{k}^{-1}+\\nu_{k}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{k}\\right)^{\\mathrm{T}} \\mathbf{W}_{k}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{k}\\right)$. The proof of this is fiddly but it does work (see https://github.com/zhengqigao/PRML-Solution-Manual, p209).\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTWjrOm7i9Kc"
      },
      "source": [
        "def E_ln_pi_k(k, alpha):\r\n",
        "  return digamma(alpha[k]) - digamma(np.sum(alpha))\r\n",
        "\r\n",
        "def E_ln_lam_k(k, nu, W):\r\n",
        "  return np.sum(digamma(nu[k]+1-np.arange(D)+1)) + D*np.log(2) + np.log(det(W[k]))\r\n",
        "\r\n",
        "def E_ln_mu_k(k, beta, m, W, nu, xn):\r\n",
        "  # print('m, W\\n', m, W)\r\n",
        "  return D*beta[k]**-1 + nu[k]*multi_dot(((xn-m[k]), W[k], (xn-m[k]).T))\r\n",
        "\r\n",
        "def ln_rho_nk(k, alpha, nu, W, beta, m, xn):\r\n",
        "  return E_ln_pi_k(k,alpha) + 0.5*E_ln_lam_k(k, nu, W) - 0.5*D*np.log(2*np.pi) - 0.5*E_ln_mu_k(k, beta, m, W, nu, xn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwTlxPVqqXtv"
      },
      "source": [
        "Now we need $r_{nk}$, which we remember is $\\rho_{nk}$ divided by the sum of $\\rho_{nk}$ over $k$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPKnSiCkqnCX"
      },
      "source": [
        "def r_nk(k, alpha, nu, W, beta, m, xn):\r\n",
        "  rhonk = np.exp(ln_rho_nk(k, alpha, nu, W, beta, m, xn))\r\n",
        "  sum_k_rho = 0.\r\n",
        "  for j in range(K):\r\n",
        "    sum_k_rho = sum_k_rho + np.exp(ln_rho_nk(j, alpha, nu, W, beta, m, xn))\r\n",
        "  return rhonk/sum_k_rho"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJFqXhTIrry3"
      },
      "source": [
        "### The algorithm\r\n",
        "E step: Calculate responsibilities $r_{nk}$ for each and every mixture component $k$ and datapoint $x_n$.\r\n",
        "\r\n",
        "M step: use $r_{nk}$ in the update equations for $\\alpha, \\beta, m, W, \\nu$ to minimise the $\\mathbb{KL}$ divergence between the variational distribution $q(Z,\\pi,\\mu,\\Lambda)$ and the true posterior $p(Z,\\pi,\\mu,\\Lambda|X)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM-wW0Cxv6xh"
      },
      "source": [
        "# E step: calculate responsibility\r\n",
        "def E_step(N,K,alpha,nu,W,beta,m,X):\r\n",
        "  r = []\r\n",
        "  for k in range(K):\r\n",
        "    r.append([])\r\n",
        "    for n in range(N):\r\n",
        "      r[k].append(r_nk(k, alpha, nu, W, beta, m, X[n]))\r\n",
        "  return r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5RMJOI9zwBv"
      },
      "source": [
        "# M step: update hyperparameters\r\n",
        "def M_step(r,X,alpha0,beta0,m0,W0,nu0):\r\n",
        "  NK, xbar, S = [],[],[]\r\n",
        "  alpha, beta, nu = np.empty(K), np.empty(K), np.empty(K)\r\n",
        "  m, W = [np.zeros(2) for _ in range(K)], [np.zeros((2,2)) for _ in range(K)]\r\n",
        "\r\n",
        "  for k in range(K):\r\n",
        "    Nk = N_k(r[k])\r\n",
        "    xkbar = x_k_bar(Nk, r[k], X)\r\n",
        "    Sk = S_k(Nk, r[k], X, xkbar)\r\n",
        "\r\n",
        "    alpha[k] = alpha_k(Nk, alpha0)\r\n",
        "    beta[k] = beta_k(Nk, beta0)\r\n",
        "    m[k] = m_k(Nk, xkbar, beta[k], m0, beta0)\r\n",
        "    W[k] = W_k(Nk, xkbar, Sk, m0, beta0)\r\n",
        "    nu[k] = nu_k(Nk, nu0)\r\n",
        "\r\n",
        "    NK.append(Nk)\r\n",
        "    xbar.append(xkbar)\r\n",
        "    S.append(Sk)\r\n",
        "\r\n",
        "    # print('k=%d\\nalpha'%k, alpha[k], '\\nbeta', beta[k], '\\nm', m[k], '\\nW', W[k], '\\nnu', nu[k])\r\n",
        "  return alpha, beta, m, W, nu, NK, xbar, S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20lMlt1JE_bz"
      },
      "source": [
        "## Calculating ELBO\r\n",
        "Now lets get the programme to calculate the evidence lower bound on each iteration. From Bishop, eq.10.70:\r\n",
        "\r\n",
        "\\begin{aligned}\r\n",
        "\\mathcal{L}=& \\sum_{\\mathbb{Z}} \\iiint q(\\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda}) \\ln \\left\\{\\frac{p(\\mathbf{X}, \\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})}{q(\\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})}\\right\\} \\mathrm{d} \\boldsymbol{\\pi} \\mathrm{d} \\boldsymbol{\\mu} \\mathrm{d} \\boldsymbol{\\Lambda} \\\\\r\n",
        "=& \\mathbb{E}[\\ln p(\\mathbf{X}, \\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})]-\\mathbb{E}[\\ln q(\\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})] \\\\\r\n",
        "=& \\mathbb{E}[\\ln p(\\mathbf{X} \\mid \\mathbf{Z}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})]+\\mathbb{E}[\\ln p(\\mathbf{Z} \\mid \\boldsymbol{\\pi})]+\\mathbb{E}[\\ln p(\\boldsymbol{\\pi})]+\\mathbb{E}[\\ln p(\\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})] \\\\\r\n",
        "&-\\mathbb{E}[\\ln q(\\mathbf{Z})]-\\mathbb{E}[\\ln q(\\boldsymbol{\\pi})]-\\mathbb{E}[\\ln q(\\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})]\r\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOWD_aK0DFH-"
      },
      "source": [
        "# components of ELBO\r\n",
        "\r\n",
        "def E_ln_p_X_given_Z_mu_lam(beta, m, W, nu, NK, S, xbar):\r\n",
        "  sum = 0.\r\n",
        "  for k in range(K):\r\n",
        "    Eln_lam = E_ln_lam_k(k, nu, W)\r\n",
        "    Tk = nu[k]*np.trace(np.dot(S[k],W[k]))\r\n",
        "    Eln_mu = E_ln_mu_k(k,beta,m,W,nu,xbar[k])\r\n",
        "    # print('\\n%d\\nEln_lam: '%k, Eln_lam)\r\n",
        "    # print('Tl: ', Tk)\r\n",
        "    # print('Eln_mu: ', Eln_mu)\r\n",
        "    # print('NK: ', NK)\r\n",
        "    sum = sum + NK[k]*(Eln_lam - Eln_mu - Tk - D*np.log(2*np.pi))\r\n",
        "  #print(sum)\r\n",
        "  #print(sum[0][0])\r\n",
        "  return 0.5*sum\r\n",
        "\r\n",
        "def E_ln_p_Z_given_pi(r, alpha):\r\n",
        "  sum = 0.\r\n",
        "  for n in range(N):\r\n",
        "    for k in range(K):\r\n",
        "      sum = sum + r[k][n]*E_ln_pi_k(k,alpha)\r\n",
        "  return sum\r\n",
        "\r\n",
        "def ln_C(alpha):\r\n",
        "  return gammaln(np.sum(alpha)) - np.sum(gammaln(alpha))\r\n",
        "\r\n",
        "def ln_B(W,nu):\r\n",
        "  ln_num = -0.5*nu*np.log(det(W))\r\n",
        "  ln_det_1 = 0.5*nu*D*np.log(2) - D*(D-1)*0.25*np.log(np.pi)\r\n",
        "  ln_det_2 = np.sum(gammaln(np.array([0.5*(nu+1-i) for i in range(D)])))\r\n",
        "  return  ln_num - ln_det_1 - ln_det_2\r\n",
        "\r\n",
        "def E_ln_p_pi(alpha0, alpha):\r\n",
        "  return ln_C(alpha0*np.ones(K)) + (alpha0-1)*np.sum(np.array([E_ln_pi_k(k,alpha) for k in range(K)]))\r\n",
        "\r\n",
        "def E_ln_p_mu_lam(beta, m, W, nu, m0, W0, nu0):\r\n",
        "  sum1, sum2, sum3 = 0., 0., 0.\r\n",
        "  for k in range(K):\r\n",
        "    F = beta0*E_ln_mu_k(k, beta, m, W, nu, m0)\r\n",
        "    Eln_lam = E_ln_lam_k(k, nu, W)\r\n",
        "    sum1 = sum1 + D*np.log(beta0/(2*np.pi)) + Eln_lam - F\r\n",
        "\r\n",
        "    sum2 = sum2 + 0.5*(nu0-D-1)*E_ln_lam_k(k,nu,W)\r\n",
        "    sum3 = sum3 + nu[k]*np.trace(np.dot(inv(W0),W[k]))\r\n",
        "  lnB = ln_B(W0,nu0)\r\n",
        "  return 0.5*sum1 + sum2 - 0.5*sum3 + K*lnB\r\n",
        "\r\n",
        "def E_ln_q_Z(r):\r\n",
        "  sum = 0.\r\n",
        "  for n in range(N):\r\n",
        "    for k in range(K):\r\n",
        "      if r[k][n] > 0: # n.b. returns nan for 0log0, so need to bypass\r\n",
        "        sum = sum + r[k][n]*np.log(r[k][n])\r\n",
        "  return sum\r\n",
        "\r\n",
        "def E_ln_q_pi(alpha):\r\n",
        "  return np.sum(np.array([(alpha[k]-1)*E_ln_pi_k(k,alpha) for k in range(K)])) + ln_C(alpha)\r\n",
        "\r\n",
        "def H_q_lam_k(k,nu,W):\r\n",
        "  return -ln_B(W[k],nu[k]) - 0.5*(nu[k]-D-1)*E_ln_lam_k(k, nu, W) + 0.5*nu[k]*D\r\n",
        "\r\n",
        "def E_ln_q_mu_lam(beta,W,nu):\r\n",
        "  sum = 0.\r\n",
        "  for k in range(K):\r\n",
        "    sum=sum+0.5*E_ln_lam_k(k,nu,W)+0.5*D*np.log(beta[k]/(2*np.pi))-0.5*D-H_q_lam_k(k,nu,W)\r\n",
        "  return sum\r\n",
        "\r\n",
        "def calculate_ELBO(r,alpha,beta,m,W,nu,NK,S,xbar,alpha0,m0,W0,nu0):\r\n",
        "  p1 = E_ln_p_X_given_Z_mu_lam(beta, m, W, nu, NK, S, xbar)\r\n",
        "  p2 = E_ln_p_Z_given_pi(r, alpha)\r\n",
        "  p3 = E_ln_p_pi(alpha0, alpha)\r\n",
        "  p4 = E_ln_p_mu_lam(beta, m, W, nu, m0, W0, nu0) # this one is an array\r\n",
        "  q1 = E_ln_q_Z(r)\r\n",
        "  q2 = E_ln_q_pi(alpha)\r\n",
        "  q3 = E_ln_q_mu_lam(beta,W,nu)\r\n",
        "  # print(type(p1),type(p2),type(p3),type(p4),type(q1),type(q2),type(q3))\r\n",
        "  # print(p1)\r\n",
        "  # if np.sum(np.isnan([p1,p2,p3,p4,q1,q2,q3]))>0:\r\n",
        "  #   print('\\nArgs of calculate ELBO:\\nr,alpha,beta,m,W,nu,NK,S,xbar,alpha0,m0,W0,nu0')\r\n",
        "  #   print(r,alpha,beta,m,W,nu,NK,S,xbar,alpha0,m0,W0,nu0)\r\n",
        "  # assert not np.isnan(p1)\r\n",
        "  # assert not np.isnan(p2)\r\n",
        "  # assert not np.isnan(p3)\r\n",
        "  # assert not np.isnan(p4)\r\n",
        "  # assert not np.isnan(q1)\r\n",
        "  # assert not np.isnan(q2)\r\n",
        "  # assert not np.isnan(q3)\r\n",
        "    \r\n",
        "  return p1+p2+p3+p4-q1-q2-q3\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRiHZuBytQjk"
      },
      "source": [
        "# Code\r\n",
        "\r\n",
        "\r\n",
        "1.   Choose some priors, $\\mathbf{\\theta}_0$, set $\\mathbf{\\theta} = \\mathbf{\\theta}_0$ \r\n",
        "2.   Iterate until converged:   \r\n",
        "  1.   Calculate the full set of $NK$ responsibilities\r\n",
        "  2.   Using update equations, update each $\\theta_k$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCiKHzANutYr"
      },
      "source": [
        "from numpy.random import multivariate_normal\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "if not os.path.exists('plots'):\r\n",
        "  os.mkdir('plots')\r\n",
        "if not os.path.exists('gifs'):\r\n",
        "  os.mkdir('gifs')\r\n",
        "\r\n",
        "# Set priors, dataset, problem\r\n",
        "D = 2       # dimensions\r\n",
        "K = 3       # number of components, unnecessary ones should go to zero\r\n",
        "N = 500     # number of points in synthetic dataset\r\n",
        "N_its = 100 # number of updates\r\n",
        "\r\n",
        "# Dataset\r\n",
        "centres = [np.array([0.,8.]), np.array([5.,0.])]\r\n",
        "covs = [np.eye(2), np.array([[0.6,0.4],\r\n",
        "                             [0.4,0.6]])] \r\n",
        "X1 = multivariate_normal(mean=centres[0],\r\n",
        "                         cov=covs[0],\r\n",
        "                         size=int(N/2))\r\n",
        "X2 = multivariate_normal(mean=centres[1],\r\n",
        "                         cov=covs[1],\r\n",
        "                         size=int(N/2))\r\n",
        "X = np.concatenate((X1,X2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q_h8ZwMtOQ-"
      },
      "source": [
        "# Variational priors\r\n",
        "alpha0 = 1e-3     # as alpha0 -> 0, pi_k -> 0. As alpha0 -> Inf, pi_k -> 1/K\r\n",
        "beta0 = 1e-10     # ???\r\n",
        "m0 = np.zeros(2)  # zero by convention (symmetry)\r\n",
        "W0 = np.eye(2)    # \r\n",
        "nu0 = 2           # "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxe7om4_99bR"
      },
      "source": [
        "%matplotlib notebook\r\n",
        "# hacky, but this suppresses plots\r\n",
        "\r\n",
        "# r needs to be randomised (not uniform) because if its all the same nothing \r\n",
        "# changes - not sure of the mathematical reasoning for this\r\n",
        "r = np.array([np.random.dirichlet(np.ones(K)) for _ in range(N)])\r\n",
        "r = [r[:,k] for k in range(K)]\r\n",
        "\r\n",
        "# for plotting\r\n",
        "# alphas, betas, Ws, ms, nus = [],[],[],[],[]\r\n",
        "alphas, betas = np.zeros((N_its,K)), np.zeros((N_its,K))\r\n",
        "ms = np.zeros((N_its,K,D))\r\n",
        "varx, vary, covxy = np.empty((N_its,K)),np.empty((N_its,K)),np.empty((N_its,K)) \r\n",
        "\r\n",
        "# neaten up the output\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import warnings\r\n",
        "warnings.simplefilter('ignore')\r\n",
        "\r\n",
        "verbose = False\r\n",
        "\r\n",
        "ELBO, ELBO_M, ELBO_E = np.empty(2*N_its), np.empty(N_its), np.empty(N_its)\r\n",
        "for i in tqdm(range(N_its)):\r\n",
        "  # M step\r\n",
        "  alpha, beta, m, W, nu, NK, xbar, S = M_step(r,X,alpha0,beta0,m0,W0,nu0)\r\n",
        "  # alphas[i],betas[i],ms[i],Ws[i],nus[i] = alpha,beta,m,W,nu\r\n",
        "  alphas[i,:] = alpha\r\n",
        "  betas[i,:] = beta\r\n",
        "  # m is shape K,D when cast to nparray\r\n",
        "  ms[i,:,:] = np.array(m) # shape N_its*K*D\r\n",
        "  varx[i,:] = np.array([inv(S[k])[0,0] for k in range(K)]) \r\n",
        "  vary[i,:] = np.array([inv(S[k])[1,1] for k in range(K)]) \r\n",
        "  covxy[i,:] = np.array([inv(S[k])[1,0] for k in range(K)]) \r\n",
        "\r\n",
        "  def E_pi(alpha, alpha0, N):\r\n",
        "    return [(alpha[k])/(K*alpha0 + N) for k in range(K)]\r\n",
        "  \r\n",
        "  Epi = E_pi(alpha, alpha0, N)\r\n",
        "  ELBO[2*i] = calculate_ELBO(r,alpha,beta,m,W,nu,NK,S,xbar,alpha0,m0,W0,nu0)\r\n",
        "  ELBO_M[i] = ELBO[2*i]\r\n",
        "  \r\n",
        "  if verbose:\r\n",
        "    print('\\n******************Iteration %d************************\\n'%i)\r\n",
        "    print('\\nalpha', alpha, '\\nbeta', beta, '\\nnu', nu, '\\nm', m, '\\nW', W, '\\nnu', nu)\r\n",
        "    print('E[pi] = ', Epi)\r\n",
        "    print('ELBO = %f'%ELBO[i])\r\n",
        "\r\n",
        "  # Plot\r\n",
        "  title = 'iteration %d' % i\r\n",
        "  filename = 'plots/img%04d.png'%i\r\n",
        "  # plot_GMM(X, mu, lam, pi, centres, covs, K, title)\r\n",
        "  plot_GMM(X, m, inv(S), Epi, centres, covs, K, title, savefigpath=filename)\r\n",
        "\r\n",
        "  # E step\r\n",
        "  r = E_step(N,K,alpha,nu,W,beta,m,X)\r\n",
        "  ELBO[2*i+1] = calculate_ELBO(r,alpha,beta,m,W,nu,NK,S,xbar,alpha0,m0,W0,nu0)\r\n",
        "  ELBO_E[i] = ELBO[2*i+1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tclCYf4XAFuV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE8Fw21lEXkc"
      },
      "source": [
        "# Make and display gif \r\n",
        "filedir = 'plots'\r\n",
        "gifdir = 'gifs'\r\n",
        "gifname = make_gif(filedir, gifdir)\r\n",
        "\r\n",
        "# delete pngs for next run\r\n",
        "for file in os.listdir(filedir):\r\n",
        "  os.remove(os.path.join(filedir,file))\r\n",
        "\r\n",
        "Image(open(gifdir+\"/%s.gif\"%gifname,'rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOFBbcpkmAPf"
      },
      "source": [
        "##Plot everything else\r\n",
        "Lets see how alpha, beta, m, W, nu change over iterations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xIbV4UoCi5X"
      },
      "source": [
        "%matplotlib inline\r\n",
        "fig=plt.figure(figsize=(14,6), dpi= 100, facecolor='w', edgecolor='k')\r\n",
        "plt.plot(np.arange(0,N_its,0.5), ELBO)\r\n",
        "plt.plot(ELBO_M)\r\n",
        "plt.plot(np.arange(N_its)+0.5, ELBO_E)\r\n",
        "plt.legend(['overall', 'after M step', 'after E step'])\r\n",
        "plt.xlabel('Iterations')\r\n",
        "plt.ylabel('Evidence lower bound')\r\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mALPrjAtl-of"
      },
      "source": [
        "%matplotlib inline\r\n",
        "legends = []\r\n",
        "for k in range(K):\r\n",
        "  plt.plot(alphas[:,k].T)\r\n",
        "  legends.append('k=%d'%k)\r\n",
        "plt.legend(legends)\r\n",
        "plt.title('alphas')\r\n",
        "plt.xlabel('Iterations')\r\n",
        "plt.ylabel('alpha')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxEt6MSCo80g"
      },
      "source": [
        "%matplotlib inline\r\n",
        "legends = []\r\n",
        "for k in range(K):\r\n",
        "  plt.plot(betas[:,k].T)\r\n",
        "  legends.append('k=%d'%k)\r\n",
        "plt.legend(legends)\r\n",
        "plt.title('betas')\r\n",
        "plt.xlabel('Iterations')\r\n",
        "plt.ylabel('beta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxe-NEzxpl0-"
      },
      "source": [
        "%matplotlib inline\r\n",
        "fig=plt.figure(figsize=(14,10), dpi= 100, facecolor='w', edgecolor='k')\r\n",
        "legends = []\r\n",
        "for k in range(K):\r\n",
        "  # ms is N_its*K*D \r\n",
        "  plt.plot(ms[:,k,0], ms[:,k,1])\r\n",
        "  legends.append('k=%d'%k)\r\n",
        "  startx, starty, endx, endy = ms[0,k,0],ms[0,k,1],ms[-1,k,0],ms[-1,k,1]\r\n",
        "  plt.text(startx, starty, '0')\r\n",
        "  plt.text(endx, endy, '%d'%N_its)\r\n",
        "plt.legend(legends)\r\n",
        "plt.title('m')\r\n",
        "plt.plot(np.array([centres[0][0],centres[1][0]]),\r\n",
        "             np.array([centres[0][1],centres[1][1]]),\r\n",
        "             'rx')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WXnBSO3tHe_"
      },
      "source": [
        "%matplotlib inline\r\n",
        "legends = []\r\n",
        "for k in range(K):\r\n",
        "  plt.figure()\r\n",
        "  plt.plot(varx[:,k].T, 'r')\r\n",
        "  plt.plot(vary[:,k].T, 'b')\r\n",
        "  plt.plot(covxy[:,k].T, 'g')\r\n",
        "  legends += ['var_x', 'var_y', 'cov_xy']\r\n",
        "  plt.legend(legends)\r\n",
        "  plt.title('Covariance matrix k=%d'%k)\r\n",
        "  plt.xlabel('Iterations')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu-4rVputHm5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39i5yDnsaqVO"
      },
      "source": [
        "## Gradient ascent optimisation of ELBO (MOVED TO SPYDER)\r\n",
        "\r\n",
        "From Bishop:\r\n",
        "\r\n",
        "*Finally, it is worth noting that the lower bound provides an alternative approach for deriving the variational re-estimation equations ... To do\r\n",
        "this we use the fact that, since the model has conjugate priors, the functional form of the factors in the variational posterior distribution is known, namely discrete for $Z$, Dirichlet for $\\pi$, and Gaussian-Wishart for $(\\mu_k,\\lambda_k)$. By taking general parametric forms for these distributions we can derive the form of the lower bound as a function of the parameters of the distributions. Maximizing the bound with respect to these parameters then gives the required re-estimation equations.*\r\n",
        "\r\n",
        "i.e. ELBO is a function of the distributional parameters $\\mathbf{\\theta} = \\alpha,\\beta,m,W,\\nu$. Therefore we can calculate $\\nabla_\\theta(ELBO)$ and perform gradient ascent to maximise it and find the variational parameters of $q$, rather than using iterative update equations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amLCfsTWNtvv"
      },
      "source": [
        "\r\n",
        "def ELBO_theta(alpha,beta,m,W,nu,X,alpha0,m0,W0,nu0):\r\n",
        "  r, NK  = [],[]\r\n",
        "  xbar, S = [],[] \r\n",
        "  for k in range(K):\r\n",
        "    r.append([])\r\n",
        "    for n in range(N):\r\n",
        "      r[k].append(r_nk(k, alpha, nu, W, beta, m, X[n]))\r\n",
        "  for k in range(K):\r\n",
        "    NK.append(N_k(r[k]))\r\n",
        "    xbar.append(x_k_bar(NK[k], r[k], X))\r\n",
        "    S.append(S_k(NK[k], r[k], X, xbar[k]))\r\n",
        "  return calculate_ELBO(r, alpha, beta, m, W, nu, NK, S, xbar, alpha0, m0, W0, nu0)\r\n",
        "\r\n",
        "# ELBO_theta(alpha,beta,m,W,nu,X,alpha0,m0,W0,nu0)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROpPQulBtQCl"
      },
      "source": [
        "The problem at the moment is that autograd does not support indexing, e.g. `r[n,k] = XXX`. I have instead swapped to `r[k][n]`, i.e. a list of K lists of length N, building it by `.append` rather than assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhXzJdILPa5_"
      },
      "source": [
        "%%capture\r\n",
        "# setup stuff\r\n",
        "from autograd import grad\r\n",
        "D_alpha = grad(ELBO_theta, 0)\r\n",
        "D_beta = grad(ELBO_theta, 1)\r\n",
        "D_m = grad(ELBO_theta, 2)\r\n",
        "# D_W = grad(ELBO_theta, 3)\r\n",
        "\r\n",
        "r = np.array([np.random.dirichlet(np.ones(K)) for _ in range(N)])\r\n",
        "r = [r[:,k] for k in range(K)]\r\n",
        "alpha, beta, m, W, nu, NK, xbar, S = M_step(r,X,alpha0,beta0,m0,W0,nu0)\r\n",
        "\r\n",
        "N_its = 10\r\n",
        "step_alpha, step_beta, step_m, step_W = 1.0, 1.0, .1, .1\r\n",
        "ELBO = np.empty(N_its+1)\r\n",
        "ELBO[0] = ELBO_theta(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4TG-4XoMiTZ"
      },
      "source": [
        "# Iterate\r\n",
        "for i in range(N_its):\r\n",
        "  print('\\n*****************')\r\n",
        "\r\n",
        "  d_alpha = D_alpha(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "  d_beta = D_beta(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "  d_m = D_m(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)  \r\n",
        "  # d_W = D_W(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "  \r\n",
        "  try: d_alpha = d_alpha._value\r\n",
        "  except: pass\r\n",
        "  try: d_beta = d_beta._value\r\n",
        "  except: pass\r\n",
        "  try: d_m = d_m._value\r\n",
        "  except: print('not an arraybox i guess: ', type(d_m))\r\n",
        "  # try: d_W = d_W._value\r\n",
        "  # except: print('not an arraybox i guess: ', type(d_W))\r\n",
        "\r\n",
        "  alpha = alpha + step_alpha*d_alpha\r\n",
        "  beta = beta + step_beta*d_beta\r\n",
        "  m = [m[k] + step_m*d_m[k] for k in range(K)]\r\n",
        "  # W = [W[k] + step_W*d_W[k] for k in range(K)]\r\n",
        "  print('\\nalpha: ', alpha)\r\n",
        "  print('\\nbeta: ', beta)\r\n",
        "  print('\\nm: ', m)\r\n",
        "  # print('\\nW: ', W)\r\n",
        "\r\n",
        "  ELBO[i+1] = ELBO_theta(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "  print('\\n%d: '%i, ELBO[i+1]-ELBO[i], (ELBO[i+1]-ELBO[i])>0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feVo_t8rB5VM"
      },
      "source": [
        "ELBO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iGqNIYnVtLq"
      },
      "source": [
        "ELBO.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kqQ0IWIteJf"
      },
      "source": [
        "# messing around with numerical differentiation\r\n",
        "\r\n",
        "def D_alpha(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0):\r\n",
        "  d = 1e-2\r\n",
        "  alphapl = alpha+d\r\n",
        "  alphami = alpha-d\r\n",
        "  Lpl = ELBO_theta(alphapl, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "  Lmi = ELBO_theta(alphami, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "  return (Lpl-Lmi)/(2*d)\r\n",
        "\r\n",
        "def D_m(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0):\r\n",
        "  d = 1e-2\r\n",
        "  m_pl = [m[k]+d for k in range(K)]\r\n",
        "  m_mi = [m[k]-d for k in range(K)]\r\n",
        "  Lpl = ELBO_theta(alpha, beta, m_pl, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "  Lmi = ELBO_theta(alpha, beta, m_mi, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "  return (Lpl-Lmi)/(2*d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47wkKeeFeUYb"
      },
      "source": [
        "# it doesnt work because you need the gradient for each K component. in fact since m is D*K\r\n",
        "# so really you need to work out grad for a lot of fucking paremeters - hate to say it but\r\n",
        "# autograd is going to be a necessity\r\n",
        "\r\n",
        "# r = np.array([np.random.dirichlet(np.ones(K)) for _ in range(N)])\r\n",
        "# alpha, beta, m, W, nu, NK, xbar, S = M_step(r,X,alpha0,beta0,m0,W0,nu0)\r\n",
        "\r\n",
        "# for i in range(100):\r\n",
        "#   old = ELBO_theta(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "#   d_alpha = D_alpha(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "#   d_m = D_m(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "#   print('d_m', d_m)\r\n",
        "#   alpha = alpha + d_alpha\r\n",
        "#   m = m + d_m\r\n",
        "#   new = ELBO_theta(alpha, beta, m, W, nu, X, alpha0, m0, W0, nu0)\r\n",
        "#   print('%d: '%i, new-old, (new-old)>0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qOTrTjLXQR8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}