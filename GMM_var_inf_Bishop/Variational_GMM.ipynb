{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational GMM.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpbng2y3Jrmt6Gh+3OXvu4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenGutteridge/4YP/blob/master/GMM_var_inf_Bishop/Variational_GMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEo-pk9YGmIj"
      },
      "source": [
        "This notebook is based on Chapter 9 (?) of Bishop, and aims to replicate the 'variational mixture of Gaussians' example worked through there. This is to help my understanding of this example, as much of it is the basis for the gradient ascent approach used in my 4YP, and to hopefully get functioning posterior inference. I'm stuck in a rut, and better to go from a working model than try and debug a crummy bloated one built from scratch that has never worked!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEaecN-lCF3V"
      },
      "source": [
        "# To do\r\n",
        "- combine the crap together into a utils file\r\n",
        "- clean up explanation\r\n",
        "- get the code nice and together\r\n",
        "- get rid of the sample stuff for the moment\r\n",
        "\r\n",
        "to clear up:\r\n",
        "- why use Sk and m for plots?\r\n",
        "- why do we need to sample r from a dirichlet to start with?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB2Ryd0bGRL7"
      },
      "source": [
        "# Intro\r\n",
        "Distribution of $Z$:\r\n",
        "\r\n",
        "i.e. the assignment $z_i$ for a single $x_i$ equals $k$ with probability $\\pi_k$, where $\\sum \\limits^K \\pi_k = 1$\r\n",
        "\r\n",
        "$p(Z|\\pi) = \\prod \\limits^N \\prod \\limits^K \\pi_{k}^{z_{nk}}$\r\n",
        "\r\n",
        "Conditional distribution of observed data (i.e. dataset given true labels and Gaussian means/precisions):\r\n",
        "\r\n",
        "$p(X|Z,\\mu,\\Lambda) = \\prod \\limits^N \\prod \\limits^K \\mathcal{N}(x_n | \\mu_k, \\Lambda_k^{-1})^{z_{nk}}$\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bPHzCamh00L"
      },
      "source": [
        "#### The joint distribution\r\n",
        "\r\n",
        "Factorises as follows:\r\n",
        "\r\n",
        "$p(X,Z,\\pi,\\mu,\\Lambda) = p(X|Z,\\mu,\\Lambda)p(Z|\\pi)p(\\pi)p(\\mu|\\Lambda)p(\\Lambda)$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfSuheDXiV1Z"
      },
      "source": [
        "#### The variational distribution\r\n",
        "\r\n",
        "Takes the following form - the only assumption is that it factorises between latent variablesd $Z$ and the parameters:\r\n",
        "\r\n",
        "$q(Z,\\pi,\\mu,\\Lambda) = q(Z)q(\\pi,\\mu,\\Lambda)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z1Knjwxi7GD"
      },
      "source": [
        "The fundamental general expression for the iterative variaitional approach here is:\r\n",
        "\r\n",
        "$\\log q_j^*(Z_j) = \\mathbb{E}_{i \\neq j}[\\log p(X,Z)] + c$\r\n",
        "\r\n",
        "i.e. look at the expectation of the joint of $\\log p$ for all the latent variables except the one we are predicting - in our case for considering just $Z$ this is:\r\n",
        "\r\n",
        "$\\log q^*(Z) = \\mathbb{E}_{\\pi,\\mu,\\Lambda}[\\log p(X,Z,\\pi,\\mu,\\Lambda)] + c$\r\n",
        "\r\n",
        "(N.b. when we write $\\mathbb{E}_{\\pi,\\mu,\\Lambda}$, what this really is is $\\mathbb{E}_{q(\\pi,\\mu,\\Lambda)}$)\r\n",
        "\r\n",
        "And then decompose the joint into $\\mathbb{E}_\\pi[\\log p(Z|\\pi)] + \\mathbb{E}_{\\mu,\\Lambda}[\\log p(X|Z,\\mu,\\Lambda)] + c$\r\n",
        "\r\n",
        "Some reworking and taking the exponent again brings us to:\r\n",
        "\r\n",
        "$q^*(Z) = \\prod \\limits^N \\prod \\limits^K r_{nk}^{z_{nk}}$\r\n",
        "where $r_{nk} = \\dfrac{\\rho_{nk}}{\\sum \\limits^K_{j=1} \\rho_{nj}}$ and $\\rho_{nk} = \\pi_k \\mathcal{N}(x_n|\\mu,\\Lambda_k^{-1})$\r\n",
        "\r\n",
        "i.e. we have derived that the form of the $Z$ factor of the variaitonal distribution takes the form of a multinomial, with a responsibility of each mixture component $k$ for each datapoint $n$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxS9aLnxnMpS"
      },
      "source": [
        "This still leaves the other factor of the variational distribution, $q(\\pi,\\mu,\\Lambda)$. Again using the general idea of the expectation over the joint,\r\n",
        "\r\n",
        "$\\log q^*(\\pi,\\mu,\\Lambda) = \\mathbb{E}_Z[\\log p(X,Z,\\pi,\\mu,\\Lambda)] =\\\\ \\mathbb{E}_Z[\\log p(X|Z,\\mu,\\Lambda) + \\log p(Z|\\pi) + \\log p(\\pi) + \\log p(\\mu,\\Lambda)]$\r\n",
        "\r\n",
        "The last two terms simply equal themselves as they don't have a $Z$ component (can go outside of integral and the expectiation is simply an integral over a distribution summing to 1), but the first two have an expectation.\r\n",
        "\r\n",
        "From choice of priors, $p(\\pi)$ is a Dirichlet distribution $= C(\\alpha_0)\\prod \\limits^K \\pi_k^{\\alpha_0-1}$\r\n",
        "\r\n",
        "$p(\\mu,\\Lambda)$ is a factorised product of Gaussian-Wishart: $p(\\mu|\\Lambda)p(\\Lambda) = \\prod \\limits^K \\mathcal{N}(\\mu_k|m_0,(\\beta_0 \\Lambda)^{-1})\\mathcal{W}(\\Lambda_k|W_0,\\nu_0)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp2ckHl6o2tV"
      },
      "source": [
        "#### Gaussian-Wishart parameters\r\n",
        "https://math.stackexchange.com/questions/2803164/degrees-of-freedom-in-a-wishart-distribution\r\n",
        "\r\n",
        "$m_0$ is the 'location' of the mean $\\mu$ chosen usually as zero by symmetry\r\n",
        "\r\n",
        "$\\beta_0$: choosing a large $\\beta_0$ tightens the Gaussian from which $\\mu$ is sampled.\r\n",
        "\r\n",
        "$W_0$ is the positive definite scale matrix\r\n",
        "\r\n",
        "If we have some prior idea of the covariance matrix $\\Lambda^{-1}$, $\\Sigma_0$, we use $\\nu$ and $W$ to encode our confidence in it. We set $W = \\Sigma_0^{-1}$. The mean (expectation) of a Wishart is $\\nu W$ and its mode is $(\\nu-d-1) W$. the least informative prior is $\\nu=d$, or possibly $d+2$ - something small. Choosing a large $\\nu$ encodes confidence in the prior $\\Sigma_0$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hmSvmtLx3cQ"
      },
      "source": [
        "back to the variational distribution. There are clearly two parts, $q(\\pi)$ and $q(\\mu,\\Lambda)$.\r\n",
        "\r\n",
        "$q(\\pi) = \\log p(\\pi) + \\mathbb{E}_Z[\\log p(Z|\\pi)] = \\log C(\\alpha_0) + \\log(\\alpha_0 - 1) \\sum \\limits^K \\pi_k + \\sum \\limits^{N,K} \\mathbb{E}[z_{nk}] \\log \\pi_k$\r\n",
        "\r\n",
        "$\\mathbb{E}_Z[z_{nk}]$ is the expectation of a multinomial, i.e. number of trials*probability, $=r_{nk}$\r\n",
        "\r\n",
        "$q^*(\\pi) = C_1 \\prod \\limits^K \\pi_k^{\\alpha_0-1}\\prod \\limits^N \\pi_k^{r_{nk}}= C_1\\prod \\limits^K \\pi_k^{(\\alpha_0-1+\\sum \\limits^N r_{nk})} = Dir(\\pi|\\alpha)$ where $\\alpha_k = \\alpha_0 + \\sum \\limits^N r_{nk}$\r\n",
        "\r\n",
        "Cool!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji_CQVBP2qpW"
      },
      "source": [
        "And now for $\\log q^*(\\mu,\\Lambda)$:\r\n",
        "\r\n",
        "$\\log q^*(\\mu,\\Lambda) = \\sum \\limits^K \\log p(\\mu_k,\\Lambda_k) + \\mathbb{E}_Z[\\log p(X|Z,\\mu,\\Lambda)] = \\\\\r\n",
        "\\sum \\limits^K \\log (\\mathcal{N}(\\mu_k|m_0,\\beta_0,\\Lambda_k) \\mathcal{W}(\\Lambda_k|\\nu,W_0)) + \\sum \\limits^{N,K} \\mathbb{E}_Z[z_{nk}]\\log \\mathcal{N}(x_n|\\mu_k,\\Lambda_k^{-1})$\r\n",
        "\r\n",
        "Similar to q*(\\pi) this turns into a form of a Gaussian-Wishart with different parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FadB5OQ7C7Uw"
      },
      "source": [
        "import numpy as np\r\n",
        "from numpy.linalg import inv, det, multi_dot\r\n",
        "import scipy\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pzdxd2i6OXa"
      },
      "source": [
        "def alpha_k(Nk, alpha0):\r\n",
        "  return alpha0 + Nk\r\n",
        "\r\n",
        "def beta_k(Nk, beta0):\r\n",
        "  return beta0 + Nk\r\n",
        "\r\n",
        "def m_k(Nk, xkbar, betak, m0, beta0):\r\n",
        "  return ((1/betak)*(beta0*m0 + Nk*xkbar)).reshape(2)\r\n",
        "\r\n",
        "def W_k(Nk, xkbar, Sk, m0, beta0):\r\n",
        "  inv_Wk = inv(W0) + Nk*Sk + ((beta0*Nk)/(beta0+Nk))*np.dot((xkbar-m0),(xkbar-m0).T) \r\n",
        "  return inv(inv_Wk)\r\n",
        "\r\n",
        "def nu_k(Nk, nu0):\r\n",
        "  return nu0 + Nk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVWHggFG7svk"
      },
      "source": [
        "These use some new parameters, $N_k, \\bar{x}_k, S_k$, which essentially correspond to the sum of responsibilities, responsibility-weighted mean and responsibility-weighted variance for mixture component $k$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN_ARihE8Jby"
      },
      "source": [
        "def N_k(responsibilities_k):\r\n",
        "  return np.sum(responsibilities_k)\r\n",
        "\r\n",
        "def x_k_bar(Nk, responsibilities_k, X):\r\n",
        "  N = responsibilities_k.shape[0]\r\n",
        "  D = X.shape[1]\r\n",
        "  sum = np.zeros((1,D))\r\n",
        "  for n in range(N):\r\n",
        "    sum = sum + responsibilities_k[n]*X[n]\r\n",
        "  if Nk > 0:\r\n",
        "    return (1/Nk)*sum\r\n",
        "  else:\r\n",
        "    return 0.\r\n",
        "\r\n",
        "def S_k(Nk, responsibilities_k, X, xkbar):\r\n",
        "  N = responsibilities_k.shape[0]\r\n",
        "  sum = 0.0\r\n",
        "  for n in range(N):\r\n",
        "    sum = sum + responsibilities_k[n]*np.dot((X[n]-xkbar).T,(X[n]-xkbar))\r\n",
        "  if Nk > 0:\r\n",
        "    return (1/Nk)*sum\r\n",
        "  else:\r\n",
        "    return np.eye(2) # doesn't actually matter what it returns, the component is dead"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99bBK_oaLxix"
      },
      "source": [
        "These updates correspond to the M (maximisation) step of the EM algorithm. To perform them we of course need our responsibilities:\r\n",
        "\r\n",
        "$\\mathbb{E}_Z[r_{nk}] = \\dfrac{\\rho_{nk}}{\\sum \\limits^K_{j=1} \\rho_{nj}}$ and\r\n",
        "$\\rho_{nk} = \\pi_k \\mathcal{N}(x_n|\\mu,\\Lambda_k^{-1})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tFnG9OhQRLB"
      },
      "source": [
        "The $\\rho_{nk}$ above is not quite right --- we have ignored the $\\mathbb{E}_{\\mu,\\lambda}[.]$ Let's consider in full:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFkDw6NpOTa0"
      },
      "source": [
        "$\\begin{aligned}\r\n",
        "\\ln \\rho_{n k}=& \\mathbb{E}\\left[\\ln \\pi_{k}\\right]+\\frac{1}{2} \\mathbb{E}\\left[\\ln \\left|\\boldsymbol{\\Lambda}_{k}\\right|\\right]-\\frac{D}{2} \\ln (2 \\pi) \\\\\r\n",
        "&-\\frac{1}{2} \\mathbb{E}_{\\mu_{k}, \\Lambda_{k}}\\left[\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}} \\boldsymbol{\\Lambda}_{k}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\right)\\right]\r\n",
        "\\end{aligned}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6M2u92xQJO5"
      },
      "source": [
        "There are four components here:\r\n",
        "\r\n",
        "\r\n",
        "1.   $\\mathbb{E}[\\ln \\pi_{k}] = \\psi(\\alpha_k) - \\psi(\\sum \\limits^K \\alpha_k)$, using the digamma function $\\psi(x) = \\dfrac{d}{dx}\\log \\Gamma = \\dfrac{\\Gamma '}{\\Gamma}$. This is the known log-expectation of the Dirichlet distribution.\r\n",
        "2.   $ \\mathbb{E}\\left[\\ln \\left|\\boldsymbol{\\Lambda}_{k}\\right|\\right] = \\sum_{i=1}^{D} \\psi\\left(\\frac{\\nu_{k}+1-i}{2}\\right)+D \\ln 2+\\ln \\left|\\mathbf{W}_{k}\\right|$. This is the known log-expectation of the Wishart distribution.\r\n",
        "3. Easy\r\n",
        "4. $\\mathbb{E}_{\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k}}\\left[\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}} \\boldsymbol{\\Lambda}_{k}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\right)\\right] = D\\beta_{k}^{-1}+\\nu_{k}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{k}\\right)^{\\mathrm{T}} \\mathbf{W}_{k}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{k}\\right)$. The proof of this is fiddly but it does work (see https://github.com/zhengqigao/PRML-Solution-Manual, p209).\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTWjrOm7i9Kc"
      },
      "source": [
        "from scipy.special import digamma\r\n",
        "def ln_rho_nk(k, alpha, nu, W, beta, m, xn):\r\n",
        "  D = X.shape[1] # dimensionality\r\n",
        "  \r\n",
        "  E_ln_pi_k =  digamma(alpha[k]) - digamma(np.sum(alpha))\r\n",
        "  E_ln_lam_k = np.sum(digamma(nu[k]+1-np.arange(D)+1)) + D*np.log(2) + np.log(det(W[k]))\r\n",
        "  E_ln_mu_k = D*beta[k]**-1 + nu[k]*multi_dot(((xn-m[k]).T, W[k], (xn-m[k])))\r\n",
        "  \r\n",
        "  return E_ln_pi_k + 0.5*E_ln_lam_k - 0.5*D*np.log(2*np.pi) - 0.5*E_ln_mu_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwTlxPVqqXtv"
      },
      "source": [
        "Now we need $r_{nk}$, which we remember is $\\rho_{nk}$ divided by the sum of $\\rho_{nk}$ over $k$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPKnSiCkqnCX"
      },
      "source": [
        "def r_nk(k, alpha, nu, W, beta, m, xn):\r\n",
        "  rhonk = np.exp(ln_rho_nk(k, alpha, nu, W, beta, m, xn))\r\n",
        "  sum_k_rho = 0.\r\n",
        "  for j in range(K):\r\n",
        "    sum_k_rho += np.exp(ln_rho_nk(j, alpha, nu, W, beta, m, xn))\r\n",
        "  return rhonk/sum_k_rho"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJFqXhTIrry3"
      },
      "source": [
        "### The algorithm\r\n",
        "E step: Calculate responsibilities $r_{nk}$ for each and every mixture component $k$ and datapoint $x_n$.\r\n",
        "\r\n",
        "M step: use $r_{nk}$ in the update equations for $\\alpha, \\beta, m, W, \\nu$ to minimise the $\\mathbb{KL}$ divergence between the variational distribution $q(Z,\\pi,\\mu,\\Lambda)$ and the true posterior $p(Z,\\pi,\\mu,\\Lambda|X)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM-wW0Cxv6xh"
      },
      "source": [
        "# E step: calculate responsibility\r\n",
        "def E_step(N,K,alpha,nu,W,beta,m,X):\r\n",
        "  r = np.empty((N,K))\r\n",
        "  for n in range(N):\r\n",
        "    for k in range(K):\r\n",
        "      r[n,k] = r_nk(k, alpha, nu, W, beta, m, X[n])\r\n",
        "  return r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5RMJOI9zwBv"
      },
      "source": [
        "# M step: update hyperparameters\r\n",
        "def M_step(r,X,alpha0,beta0,m0,W0,nu0):\r\n",
        "  NK, xbar, S = [],[],[]\r\n",
        "  alpha, beta, nu = np.empty(K), np.empty(K), np.empty(K)\r\n",
        "  m, W = [np.zeros(2) for _ in range(K)], [np.zeros((2,2)) for _ in range(K)]\r\n",
        "\r\n",
        "  for k in range(K):\r\n",
        "    Nk = N_k(r[:,k])\r\n",
        "    print('N_%d = '%k, Nk)\r\n",
        "    xkbar = x_k_bar(Nk, r[:,k], X)\r\n",
        "    Sk = S_k(Nk, r[:,k], X, xkbar)\r\n",
        "\r\n",
        "    alpha[k] = alpha_k(Nk, alpha0)\r\n",
        "    beta[k] = beta_k(Nk, beta0)\r\n",
        "    m[k] = m_k(Nk, xkbar, beta[k], m0, beta0)\r\n",
        "    W[k] = W_k(Nk, xkbar, Sk, m0, beta0)\r\n",
        "    nu[k] = nu_k(Nk, nu0)\r\n",
        "\r\n",
        "    NK.append(Nk)\r\n",
        "    xbar.append(xkbar)\r\n",
        "    S.append(Sk)\r\n",
        "\r\n",
        "    # print('k=%d\\nalpha'%k, alpha[k], '\\nbeta', beta[k], '\\nm', m[k], '\\nW', W[k], '\\nnu', nu[k])\r\n",
        "  return alpha, beta, m, W, nu, NK, xbar, S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRiHZuBytQjk"
      },
      "source": [
        "# Code\r\n",
        "\r\n",
        "\r\n",
        "1.   Choose some priors, $\\mathbf{\\theta}_0$, set $\\mathbf{\\theta} = \\mathbf{\\theta}_0$ \r\n",
        "2.   Iterate until converged:   \r\n",
        "  1.   Calculate the full set of $NK$ responsibilities\r\n",
        "  2.   Using update equations, update each $\\theta_k$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCiKHzANutYr"
      },
      "source": [
        "from numpy.random import multivariate_normal\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Set priors, dataset, problem\r\n",
        "D = 2       # dimensions\r\n",
        "K = 5       # number of components, unnecessary ones should go to zero\r\n",
        "N = 500     # number of points in synthetic dataset\r\n",
        "N_its = 100 # number of updates\r\n",
        "\r\n",
        "# Dataset\r\n",
        "centres = [np.array([0.,8.]), np.array([5.,0.])]\r\n",
        "covs = [np.eye(2), np.array([[0.6,0.4],\r\n",
        "                             [0.4,0.6]])] \r\n",
        "X1 = multivariate_normal(mean=centres[0],\r\n",
        "                         cov=covs[0],\r\n",
        "                         size=int(N/2))\r\n",
        "X2 = multivariate_normal(mean=centres[1],\r\n",
        "                         cov=covs[1],\r\n",
        "                         size=int(N/2))\r\n",
        "X = np.concatenate((X1,X2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q_h8ZwMtOQ-"
      },
      "source": [
        "# Variational priors\r\n",
        "alpha0 = 1e-3     # as alpha0 -> 0, pi_k -> 0. As alpha0 -> Inf, pi_k -> 1/K\r\n",
        "beta0 = 1e-10     # ???\r\n",
        "m0 = np.zeros(2)  # zero by convention (symmetry)\r\n",
        "W0 = np.eye(2)    # \r\n",
        "nu0 = 2           # "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxe7om4_99bR"
      },
      "source": [
        "%matplotlib notebook \r\n",
        "# hacky, but this suppresses plots\r\n",
        "\r\n",
        "# r needs to be randomised (not uniform) because if its all the same nothing \r\n",
        "# changes - not sure of the mathematical reasoning for this\r\n",
        "r = np.array([np.random.dirichlet(np.ones(K)) for _ in range(N)])\r\n",
        "\r\n",
        "# neaten up the output\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import warnings\r\n",
        "warnings.simplefilter('ignore')\r\n",
        "\r\n",
        "with HiddenPrints():\r\n",
        "# for _ in range(1):\r\n",
        "  for i in tqdm(range(N_its)):\r\n",
        "    print('\\n******************Iteration %d************************\\n'%i)\r\n",
        "    \r\n",
        "    # M step\r\n",
        "    alpha, beta, m, W, nu, NK, xbar, S = M_step(r,X,alpha0,beta0,m0,W0,nu0)\r\n",
        "    print('\\nalpha', alpha, '\\nbeta', beta, '\\nnu', nu, '\\nm', m, '\\nW', W, '\\nnu', nu)\r\n",
        "\r\n",
        "    # mu, lam = [],[]\r\n",
        "    # pi = sample_pi(alpha)\r\n",
        "    # for k in range(K):\r\n",
        "    #   lam.append(sample_lambda(W[k], nu[k]))\r\n",
        "    #   mu.append(sample_mu(m[k], beta[k], lam[k]))\r\n",
        "\r\n",
        "    def E_pi(alpha, alpha0, N):\r\n",
        "      return [(alpha[k])/(K*alpha0 + N) for k in range(K)]\r\n",
        "    \r\n",
        "    Epi = E_pi(alpha, alpha0, N)\r\n",
        "    print('E[pi] = ', Epi)\r\n",
        "\r\n",
        "    # Plot\r\n",
        "    title = 'iteration %d' % i\r\n",
        "    filename = 'plots/img%04d.png'%i\r\n",
        "    # plot_GMM(X, mu, lam, pi, centres, covs, K, title)\r\n",
        "    plot_GMM(X, m, inv(S), Epi, centres, covs, K, title, savefigpath=filename)\r\n",
        "\r\n",
        "    # E step\r\n",
        "    r = E_step(N,K,alpha,nu,W,beta,m,X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE8Fw21lEXkc"
      },
      "source": [
        "%matplotlib inline\r\n",
        "# Make and display gif \r\n",
        "filedir = 'plots'\r\n",
        "gifname = make_gif(filedir)\r\n",
        "\r\n",
        "# delete pngs for next run\r\n",
        "for file in os.listdir(filedir):\r\n",
        "  os.remove(os.path.join(filedir,file))\r\n",
        "\r\n",
        "Image(open(\"/content/%s.gif\"%gifname,'rb').read())\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20lMlt1JE_bz"
      },
      "source": [
        "## Calculating ELBO\r\n",
        "Now lets get the programme to calculate the evidence lower bound on each iteration. From Bishop, eq.10.70:\r\n",
        "\r\n",
        "\\begin{aligned}\r\n",
        "\\mathcal{L}=& \\sum_{\\mathbb{Z}} \\iiint q(\\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda}) \\ln \\left\\{\\frac{p(\\mathbf{X}, \\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})}{q(\\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})}\\right\\} \\mathrm{d} \\boldsymbol{\\pi} \\mathrm{d} \\boldsymbol{\\mu} \\mathrm{d} \\boldsymbol{\\Lambda} \\\\\r\n",
        "=& \\mathbb{E}[\\ln p(\\mathbf{X}, \\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})]-\\mathbb{E}[\\ln q(\\mathbf{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})] \\\\\r\n",
        "=& \\mathbb{E}[\\ln p(\\mathbf{X} \\mid \\mathbf{Z}, \\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})]+\\mathbb{E}[\\ln p(\\mathbf{Z} \\mid \\boldsymbol{\\pi})]+\\mathbb{E}[\\ln p(\\boldsymbol{\\pi})]+\\mathbb{E}[\\ln p(\\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})] \\\\\r\n",
        "&-\\mathbb{E}[\\ln q(\\mathbf{Z})]-\\mathbb{E}[\\ln q(\\boldsymbol{\\pi})]-\\mathbb{E}[\\ln q(\\boldsymbol{\\mu}, \\boldsymbol{\\Lambda})]\r\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOWD_aK0DFH-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}